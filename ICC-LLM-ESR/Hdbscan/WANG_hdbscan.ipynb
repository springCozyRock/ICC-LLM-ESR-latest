{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080f5d0c-b9aa-4404-837c-6b7016fba35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] (4722, 64)\n",
      "[Check] row-norm mean/min/max = 1.0000 / 1.0000 / 1.0000\n",
      "[KNN-10] 第10近邻距离 p50=0.3674  p90=0.4805\n",
      "\n",
      "=== 先试欧氏距离（原空间/L2）===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# A) 先在原空间试 Euclidean（很多数据在欧氏下更易分簇）\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 先试欧氏距离（原空间/L2）===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m leaderboard_eu, raw_grid_eu \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_cluster_size_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m direct_eu_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m leaderboard_eu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (leaderboard_eu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direct_eu_ok:\n",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(X, mcs_list, ms_list, metric)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     labels, probs, _, m \u001b[38;5;241m=\u001b[39m \u001b[43mhdbscan_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     97\u001b[0m         min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(mcs),\n\u001b[1;32m     98\u001b[0m         min_samples\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m ms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(ms)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m         avg_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(probs)) \u001b[38;5;28;01mif\u001b[39;00m probs\u001b[38;5;241m.\u001b[39msize \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    104\u001b[0m     ))\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m, in \u001b[0;36mhdbscan_clustering\u001b[0;34m(X, min_cluster_size, min_samples, metric, eval_subsample)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhdbscan_clustering\u001b[39m(X, min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_subsample\u001b[38;5;241m=\u001b[39mEVAL_SUBSAMPLE):\n\u001b[1;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mhdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHDBSCAN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_dist_n_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_single_cluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# ★ 新增：允许得到单簇，避免全噪声\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_selection_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ★ 新增：稍放松簇选择阈值\u001b[39;49;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     labels \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     64\u001b[0m     probs  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprobabilities_\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/hdbscan/hdbscan_.py:1251\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1241\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_kwargs)\n\u001b[1;32m   1242\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_min_span_tree\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_detection_data\n\u001b[1;32m   1244\u001b[0m (\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_,\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_,\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_persistence_,\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree,\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_linkage_tree,\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_spanning_tree,\n\u001b[0;32m-> 1251\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mhdbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_finite:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;66;03m# remap indices to align with original data in the case of non-finite entries.\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree \u001b[38;5;241m=\u001b[39m remap_condensed_tree(\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree, internal_to_raw, outliers\n\u001b[1;32m   1257\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/hdbscan/hdbscan_.py:841\u001b[0m, in \u001b[0;36mhdbscan\u001b[0;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, max_cluster_size, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, cluster_selection_epsilon_max, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m KDTREE_VALID_METRICS:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;66;03m# TO DO: Need heuristic to decide when to go to boruvka;\u001b[39;00m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# still debugging for now\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m60\u001b[39m:\n\u001b[0;32m--> 841\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_hdbscan_prims_kdtree\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m            \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgen_min_span_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mcache(\n\u001b[1;32m    855\u001b[0m             _hdbscan_boruvka_kdtree\n\u001b[1;32m    856\u001b[0m         )(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    867\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/hdbscan/hdbscan_.py:263\u001b[0m, in \u001b[0;36m_hdbscan_prims_kdtree\u001b[0;34m(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m dist_metric \u001b[38;5;241m=\u001b[39m DistanceMetric\u001b[38;5;241m.\u001b[39mget_metric(metric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Get distance to kth nearest neighbour\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m core_distances \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdualtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreadth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Mutual reachability distance is implicit in mst_linkage_core_vector\u001b[39;00m\n\u001b[1;32m    268\u001b[0m min_spanning_tree \u001b[38;5;241m=\u001b[39m mst_linkage_core_vector(X, core_distances, dist_metric, alpha)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === hdbscan_minimal_with_fallback_v2.py ===\n",
    "import os, json, pickle, numpy as np, pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import hdbscan\n",
    "from hdbscan import all_points_membership_vectors\n",
    "\n",
    "# ------- 轻量可调项 -------\n",
    "DATA_PATH        = 'data/fashion/handled/pca64_itm_emb_np.pkl'\n",
    "OUT_DIR          = './artifacts_hdbscan_min'\n",
    "SEED             = 42\n",
    "EVAL_SUBSAMPLE   = 8000    # 评价抽样上限（提速）\n",
    "MAX_NOISE_RATIO  = 0.5     # 搜索过滤：噪声比例上限\n",
    "MIN_N_CLUSTERS   = 2       # 略放松，便于起步（原为 3）\n",
    "USE_STANDARDIZE  = False   # ★ 可切 True 看看是否更稳（会打印“数据范围”）\n",
    "min_cluster_size_list = [3, 5, 8, 10, 12, 15, 20, 25, 30, 40, 50]\n",
    "min_samples_list      = [None, 1, 2, 3, 5, 8, 10]\n",
    "# -------------------------\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    print(f\"[Load] {X.shape}\")\n",
    "    return X\n",
    "\n",
    "def prep_features(X_raw, use_standardize=USE_STANDARDIZE):\n",
    "    # 可选：先标准化，再 L2\n",
    "    if use_standardize:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        Xs = scaler.fit_transform(X_raw)\n",
    "        print(f\"[Scale] range after StandardScaler: min={Xs.min():.3f}, max={Xs.max():.3f}\")\n",
    "        X = normalize(Xs, norm='l2', axis=1)\n",
    "    else:\n",
    "        X = normalize(X_raw, norm='l2', axis=1)\n",
    "    # 简要检查\n",
    "    rn = np.linalg.norm(X, axis=1)\n",
    "    print(f\"[Check] row-norm mean/min/max = {rn.mean():.4f} / {rn.min():.4f} / {rn.max():.4f}\")\n",
    "    # KNN 健康检查（密度差异是否存在）\n",
    "    nbrs = NearestNeighbors(n_neighbors=10, metric='cosine').fit(X)\n",
    "    dists, _ = nbrs.kneighbors(X)\n",
    "    p50 = np.percentile(dists[:, -1], 50)\n",
    "    p90 = np.percentile(dists[:, -1], 90)\n",
    "    print(f\"[KNN-10] 第10近邻距离 p50={p50:.4f}  p90={p90:.4f}\")\n",
    "    return X\n",
    "\n",
    "def hdbscan_clustering(X, min_cluster_size=25, min_samples=None, metric='cosine', eval_subsample=EVAL_SUBSAMPLE):\n",
    "    model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True,\n",
    "        core_dist_n_jobs=os.cpu_count(),\n",
    "        allow_single_cluster=True,       # ★ 新增：允许得到单簇，避免全噪声\n",
    "        cluster_selection_epsilon=0.05   # ★ 新增：稍放松簇选择阈值\n",
    "    ).fit(X)\n",
    "    labels = model.labels_.astype(np.int32)\n",
    "    probs  = model.probabilities_.astype(np.float32)\n",
    "\n",
    "    # 统计\n",
    "    uniq, cnts = np.unique(labels, return_counts=True)\n",
    "    n_clusters  = len(uniq) - (1 if -1 in uniq else 0)\n",
    "    noise_count = cnts[uniq == -1][0] if -1 in uniq else 0\n",
    "    noise_ratio = float(noise_count) / len(X)\n",
    "\n",
    "    # 指标（排噪声 + 抽样）\n",
    "    sil, cal = None, None\n",
    "    if n_clusters > 1:\n",
    "        idx = np.where(labels != -1)[0]\n",
    "        if idx.size >= 3 and np.unique(labels[idx]).size > 1:\n",
    "            if idx.size > eval_subsample:\n",
    "                idx = rng.choice(idx, size=eval_subsample, replace=False)\n",
    "            X_eval, y_eval = X[idx], labels[idx]\n",
    "            try: sil = float(silhouette_score(X_eval, y_eval, metric=metric))\n",
    "            except: sil = None\n",
    "            try: cal = float(calinski_harabasz_score(X_eval, y_eval))\n",
    "            except: cal = None\n",
    "\n",
    "    metrics = dict(n_clusters=int(n_clusters), noise_ratio=noise_ratio, silhouette=sil, calinski=cal)\n",
    "    return labels, probs, model, metrics\n",
    "\n",
    "def grid_search(X, mcs_list, ms_list, metric='cosine'):\n",
    "    rows = []\n",
    "    for mcs, ms in product(mcs_list, ms_list):\n",
    "        # 经验性约束：过大 min_samples 容易全噪声\n",
    "        if ms is not None and ms > mcs:\n",
    "            continue\n",
    "        try:\n",
    "            labels, probs, _, m = hdbscan_clustering(X, mcs, ms, metric)\n",
    "            rows.append(dict(\n",
    "                min_cluster_size=int(mcs),\n",
    "                min_samples=(None if ms is None else int(ms)),\n",
    "                N=m['n_clusters'],\n",
    "                noise_ratio=m['noise_ratio'],\n",
    "                silhouette=m['silhouette'],\n",
    "                calinski=m['calinski'],\n",
    "                avg_prob=float(np.mean(probs)) if probs.size else np.nan\n",
    "            ))\n",
    "        except Exception:\n",
    "            rows.append(dict(\n",
    "                min_cluster_size=int(mcs), min_samples=(None if ms is None else int(ms)),\n",
    "                N=np.nan, noise_ratio=1.0, silhouette=np.nan, calinski=np.nan, avg_prob=np.nan\n",
    "            ))\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # 过滤\n",
    "    valid = df[(df['noise_ratio'] <= MAX_NOISE_RATIO) & (df['N'] >= MIN_N_CLUSTERS)].copy()\n",
    "    if valid.empty:\n",
    "        print(\"[Warn] 无满足约束的组合，放宽为全部结果的排序参考。\")\n",
    "        valid = df.copy()\n",
    "\n",
    "    # 简单综合评分：sil 主导 + 低噪声 + 高 avg_prob，少量鼓励更多簇\n",
    "    def score(r):\n",
    "        s = 0.0\n",
    "        if pd.notna(r['silhouette']): s += 1.0 * r['silhouette']\n",
    "        if pd.notna(r['avg_prob']):   s += 0.2 * r['avg_prob']\n",
    "        s += 0.05 * np.log1p(max(r['N'], 0))\n",
    "        s -= 0.5 * r['noise_ratio']\n",
    "        return s\n",
    "\n",
    "    valid['score'] = valid.apply(score, axis=1)\n",
    "    valid = valid.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    return valid, df\n",
    "\n",
    "# ---------- 主流程 ----------\n",
    "X_raw = load_embeddings(DATA_PATH)\n",
    "X = prep_features(X_raw, USE_STANDARDIZE)\n",
    "\n",
    "# A) 先在原空间试 Euclidean（很多数据在欧氏下更易分簇）\n",
    "print(\"\\n=== 先试欧氏距离（原空间/L2）===\")\n",
    "leaderboard_eu, raw_grid_eu = grid_search(X, min_cluster_size_list, min_samples_list, metric='euclidean')\n",
    "direct_eu_ok = not leaderboard_eu['N'].isna().all() and not (leaderboard_eu['N'].fillna(0) == 0).all()\n",
    "\n",
    "if direct_eu_ok:\n",
    "    leaderboard, raw_grid = leaderboard_eu, raw_grid_eu\n",
    "    DIRECT_EUCLIDEAN = True\n",
    "else:\n",
    "    DIRECT_EUCLIDEAN = False\n",
    "    print(\"\\n=== 再试 Cosine（原空间/L2）===\")\n",
    "    leaderboard, raw_grid = grid_search(X, min_cluster_size_list, min_samples_list, metric='cosine')\n",
    "\n",
    "print(leaderboard.head(10))\n",
    "\n",
    "# B) 若原空间两种度量都不行，走 UMAP 兜底\n",
    "need_fallback = leaderboard['N'].isna().all() or (leaderboard['N'].fillna(0) == 0).all()\n",
    "if need_fallback:\n",
    "    print(\"\\n[Fallback] 启用 UMAP(10维, cosine) + HDBSCAN(euclidean) 兜底...\")\n",
    "    import umap\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=10,     # ★ 更高一点的低维空间\n",
    "        n_neighbors=30,      # ★ 扩大邻域，增强全局结构\n",
    "        min_dist=0.0,        # ★ 拉开簇间距\n",
    "        random_state=SEED,\n",
    "        metric='cosine'\n",
    "    )\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "    # 在低维欧式空间再搜一次（更宽松）\n",
    "    mcs_list_fb = [3, 5, 8, 10, 12, 15, 20]\n",
    "    ms_list_fb  = [None, 1, 2, 3, 5]\n",
    "    leaderboard, raw_grid = grid_search(X_umap, mcs_list_fb, ms_list_fb, metric='euclidean')\n",
    "    print(leaderboard.head(10))\n",
    "    if leaderboard.empty:\n",
    "        raise RuntimeError(\"UMAP 兜底后仍无可用最优参数，请进一步放宽搜索/检查数据。\")\n",
    "    # 后续流程切换到降维后的 X\n",
    "    X = X_umap\n",
    "\n",
    "if leaderboard.empty:\n",
    "    raise RuntimeError(\"无可用最优参数。请放宽筛选条件或调整搜索空间。\")\n",
    "\n",
    "best = leaderboard.iloc[0]\n",
    "# 根据实际走通的路径确定 metric\n",
    "if need_fallback:\n",
    "    chosen_metric = 'euclidean'\n",
    "elif 'DIRECT_EUCLIDEAN' in globals() and DIRECT_EUCLIDEAN:\n",
    "    chosen_metric = 'euclidean'\n",
    "else:\n",
    "    chosen_metric = 'cosine'\n",
    "\n",
    "best_params = dict(\n",
    "    min_cluster_size=int(best['min_cluster_size']),\n",
    "    min_samples=(None if pd.isna(best['min_samples']) else int(best['min_samples'])),\n",
    "    metric=chosen_metric,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "print(\"\\n=== 最优参数 ===\")\n",
    "print(best_params)\n",
    "\n",
    "labels, probs, model, metrics = hdbscan_clustering(\n",
    "    X,\n",
    "    min_cluster_size=best_params['min_cluster_size'],\n",
    "    min_samples=best_params['min_samples'],\n",
    "    metric=best_params['metric']\n",
    ")\n",
    "\n",
    "# 加权簇中心（权重=probabilities_），单位化（配合 cosine；若是欧氏也无害）\n",
    "centers, cid_list = [], []\n",
    "for cid in sorted(c for c in np.unique(labels) if c != -1):\n",
    "    idx = np.where(labels == cid)[0]\n",
    "    if idx.size == 0: \n",
    "        continue\n",
    "    w = probs[idx].reshape(-1, 1).astype(np.float32)\n",
    "    w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    s = np.sum(w)\n",
    "    if s <= 1e-12:\n",
    "        c = X[idx].mean(axis=0)\n",
    "    else:\n",
    "        c = (X[idx] * w).sum(axis=0) / s\n",
    "    c = c / (np.linalg.norm(c) + 1e-12)\n",
    "    centers.append(c.astype(np.float32))\n",
    "    cid_list.append(int(cid))\n",
    "centers = np.vstack(centers) if len(centers) else np.zeros((0, X.shape[1]), dtype=np.float32)\n",
    "center_cluster_ids = np.array(cid_list, dtype=np.int32)\n",
    "\n",
    "# 全簇 membership 矩阵（FDAC 直接用）\n",
    "U = all_points_membership_vectors(model).astype(np.float32)\n",
    "\n",
    "# ---------- 落盘（最小必需集） ----------\n",
    "np.save(os.path.join(OUT_DIR, \"labels.npy\"), labels.astype(np.int32))\n",
    "np.save(os.path.join(OUT_DIR, \"probs.npy\"),  probs.astype(np.float32))\n",
    "np.save(os.path.join(OUT_DIR, \"centers.npy\"), centers.astype(np.float32))\n",
    "np.save(os.path.join(OUT_DIR, \"center_cluster_ids.npy\"), center_cluster_ids)\n",
    "np.save(os.path.join(OUT_DIR, \"memberships.npy\"), U.astype(np.float32))\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"best_params.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, ensure_ascii=False, indent=2)\n",
    "with open(os.path.join(OUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n[保存完成] {OUT_DIR}\")\n",
    "print(\"  - labels.npy / probs.npy / centers.npy / center_cluster_ids.npy / memberships.npy\")\n",
    "print(\"  - best_params.json / metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c67a5-0942-439a-8168-943fad885ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
